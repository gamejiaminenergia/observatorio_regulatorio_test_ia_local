from __future__ import annotations

import json
from pathlib import Path
from typing import List

from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PlaywrightURLLoader
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field


class News(BaseModel):
    """InformaciÃ³n estructurada extraÃ­da de una noticia."""
    companies: list[str] = Field(description="Empresas, organizaciones o entidades mencionadas")
    persons: list[str] = Field(description="Personas (nombres completos) mencionadas")
    events: list[str] = Field(description="Eventos, resoluciones o acontecimientos importantes")


# ConfiguraciÃ³n
MODEL_NAME = "gpt-oss:latest"
DEFAULT_URL = "https://gestornormativo.creg.gov.co/gestor/entorno/docs/resolucion_minminas_40505_2025.htm"
CHUNK_SIZE = 2000  # Caracteres por fragmento (ajusta segÃºn tu RAM)
CHUNK_OVERLAP = 100  # Solapamiento para no perder contexto entre fragmentos


def split_text_into_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    """
    Divide el texto en fragmentos manejables con solapamiento.
    
    Args:
        text: Texto completo a dividir
        chunk_size: TamaÃ±o mÃ¡ximo de cada fragmento
        overlap: Caracteres de solapamiento entre fragmentos
        
    Returns:
        Lista de fragmentos de texto
    """
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # Retroceder para crear solapamiento
    
    return chunks


def extract_from_chunk(llm: ChatOllama, parser: PydanticOutputParser, chunk: str, chunk_num: int) -> News:
    """
    Extrae informaciÃ³n de un fragmento especÃ­fico.
    
    Args:
        llm: Modelo de lenguaje configurado
        parser: Parser de Pydantic
        chunk: Fragmento de texto a analizar
        chunk_num: NÃºmero del fragmento (para logging)
        
    Returns:
        News: InformaciÃ³n extraÃ­da del fragmento
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "Eres un analista experto que extrae informaciÃ³n estructurada de documentos. "
         "Identifica personas (nombres completos), empresas (razones sociales) y eventos relevantes. "
         "Debes responder exclusivamente con JSON vÃ¡lido que siga el siguiente esquema:\n{format_instructions}"),
        ("human",
         "Analiza el siguiente fragmento y extrae:\n"
         "- Personas: nombres de individuos mencionados\n"
         "- Empresas: organizaciones, compaÃ±Ã­as o entidades\n"
         "- Eventos: hechos, resoluciones, acuerdos o acontecimientos importantes\n\n"
         "FRAGMENTO:\n{content}")
    ]).partial(format_instructions=parser.get_format_instructions())

    chain = prompt | llm
    
    print(f"   Procesando fragmento {chunk_num}... ({len(chunk)} caracteres)")
    response = chain.invoke({"content": chunk})

    try:
        news = parser.parse(response.content)
    except Exception as parse_error:
        print(f"   âš ï¸  Advertencia: Error parseando fragmento {chunk_num}, usando valores vacÃ­os")
        news = News(companies=[], persons=[], events=[])
    
    return news


def merge_news_results(results: List[News]) -> News:
    """
    Combina mÃºltiples resultados eliminando duplicados.
    
    Args:
        results: Lista de objetos News extraÃ­dos
        
    Returns:
        News: Resultado consolidado sin duplicados
    """
    all_companies = []
    all_persons = []
    all_events = []
    
    for result in results:
        all_companies.extend(result.companies)
        all_persons.extend(result.persons)
        all_events.extend(result.events)
    
    # Eliminar duplicados preservando orden y normalizando
    def deduplicate(items: List[str]) -> List[str]:
        seen = set()
        unique = []
        for item in items:
            item_normalized = item.strip().lower()
            if item_normalized and item_normalized not in seen:
                seen.add(item_normalized)
                unique.append(item.strip())
        return unique
    
    return News(
        companies=deduplicate(all_companies),
        persons=deduplicate(all_persons),
        events=deduplicate(all_events)
    )


def run_news_extraction(url: str = DEFAULT_URL) -> News:
    """
    Extrae informaciÃ³n estructurada de una URL usando procesamiento por fragmentos.
    
    Args:
        url: URL del documento a analizar
        
    Returns:
        News: Objeto con personas, empresas y eventos extraÃ­dos
    """
    print(f"ğŸŒ Cargando contenido de: {url}")
    
    # 1ï¸âƒ£ Cargar contenido web
    loader = PlaywrightURLLoader(
        urls=[url],
        remove_selectors=["script", "style", "nav", "header", "footer", "aside", "iframe"]
    )
    docs = loader.load()
    content = docs[0].page_content if docs else ""
    
    print(f"âœ… Contenido cargado: {len(content):,} caracteres")
    
    # 2ï¸âƒ£ Dividir en fragmentos
    chunks = split_text_into_chunks(content)
    print(f"ğŸ“¦ Documento dividido en {len(chunks)} fragmentos de ~{CHUNK_SIZE:,} caracteres")
    
    # 3ï¸âƒ£ Configurar modelo
    llm = ChatOllama(
        model=MODEL_NAME,
        temperature=0,
        format="json"
    )
    parser = PydanticOutputParser(pydantic_object=News)
    
    # 4ï¸âƒ£ Procesar cada fragmento
    print("\nğŸ¤– Procesando fragmentos...")
    results = []
    
    for i, chunk in enumerate(chunks, 1):
        try:
            news = extract_from_chunk(llm, parser, chunk, i)
            results.append(news)
        except Exception as e:
            print(f"   âŒ Error en fragmento {i}: {e}")
            # Continuar con los demÃ¡s fragmentos
            continue
    
    # 5ï¸âƒ£ Unificar resultados
    print(f"\nğŸ”„ Unificando resultados de {len(results)} fragmentos...")
    final_news = merge_news_results(results)
    
    return final_news


if __name__ == "__main__":
    print("ğŸš€ Iniciando extracciÃ³n de noticias con procesamiento por fragmentos\n")
    print("ğŸ“¦ Requisitos: pip install langchain langchain-community playwright")
    print("ğŸ”§ Ejecuta: playwright install chromium\n")
    
    try:
        news = run_news_extraction()
        
        print("\n" + "="*60)
        print("ğŸ“Š RESULTADOS CONSOLIDADOS")
        print("="*60)
        
        print(f"\nğŸ‘¥ Personas ({len(news.persons)}):")
        for person in news.persons:
            print(f"  â€¢ {person}")
        
        print(f"\nğŸ¢ Empresas ({len(news.companies)}):")
        for company in news.companies:
            print(f"  â€¢ {company}")
        
        print(f"\nğŸ“… Eventos ({len(news.events)}):")
        for event in news.events:
            print(f"  â€¢ {event}")

        # Guardar resultados
        output_path = Path("data.json")
        output_path.write_text(
            json.dumps(news.dict(), ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        print(f"\nğŸ’¾ Resultados guardados en: {output_path.resolve()}")
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
